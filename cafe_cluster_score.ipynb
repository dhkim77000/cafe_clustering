{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d74143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분리된 카페 리뷰를 하나의 카페로 병합\n",
    "\n",
    "def review_sum():\n",
    "    \n",
    "    import pandas as pd\n",
    "    tokenized = pd.read_csv('../data/final (2).csv')\n",
    "    \n",
    "    # (1) 데이터 형태 변경\n",
    "    import ast\n",
    "    tokenized['contents'] = tokenized['contents'].apply(lambda x: ast.literal_eval(x))\n",
    "    tokenized['noun'] = tokenized['noun'].apply(lambda x: ast.literal_eval(x))\n",
    "    tokenized['adj'] = tokenized['adj'].apply(lambda x: ast.literal_eval(x))\n",
    "    \n",
    "    # (2) Word2Vector 적용을 위한 토큰 추출\n",
    "    tokenized['verb'] = tokenized['contents'].apply(lambda x: [w[0] for w in x if w[1]=='Verb'])\n",
    "    \n",
    "    \n",
    "    # (3) 전체 카페 리스트 dataframe 생성\n",
    "    cafe_unique = tokenized[['name', 'category', 'location', 'rating', 'rating_num', 'review_num', 'link', 'Americano']].drop_duplicates(keep='first')\n",
    "    \n",
    "    # (4) 카페별 리뷰에서 추출된 명사 토큰 합치기\n",
    "    for i in cafe_unique.index:\n",
    "        link = cafe_unique.loc[i,'link']\n",
    "        review_to_sum = tokenized[tokenized['link']==link].reset_index(drop=True)\n",
    "        result = []\n",
    "        for j in review_to_sum.index:\n",
    "            result += review_to_sum.loc[j,'noun']\n",
    "        cafe_unique.loc[i,'noun'] = str(result)\n",
    "        \n",
    "    # (5) 카페별 리뷰에서 추출된 형용사 토큰 합치기\n",
    "    for i in cafe_unique.index:\n",
    "        link = cafe_unique.loc[i,'link']\n",
    "        review_to_sum = tokenized[tokenized['link']==link].reset_index(drop=True)\n",
    "        result = []\n",
    "        for j in review_to_sum.index:\n",
    "            result += review_to_sum.loc[j,'adj']\n",
    "        cafe_unique.loc[i,'adj'] = str(result)\n",
    "    \n",
    "    # (6) 카페별 리뷰에서 추출된 동사 토큰 합치기\n",
    "    for i in cafe_unique.index:\n",
    "        link = cafe_unique.loc[i,'link']\n",
    "        review_to_sum = tokenized[tokenized['link']==link].reset_index(drop=True)\n",
    "        result = []\n",
    "        for j in review_to_sum.index:\n",
    "            result += review_to_sum.loc[j,'verb']\n",
    "        cafe_unique.loc[i,'verb'] = str(result)\n",
    "    \n",
    "    # (7) 데이터 형태 변경\n",
    "    cafe_unique['noun'] = cafe_unique['noun'].apply(lambda x: ast.literal_eval(x))\n",
    "    cafe_unique['adj'] = cafe_unique['adj'].apply(lambda x: ast.literal_eval(x))\n",
    "    cafe_unique['verb'] = cafe_unique['verb'].apply(lambda x: ast.literal_eval(x))\n",
    "    \n",
    "    # (8) 카페별 전체 토큰 합치기\n",
    "    cafe_unique['token'] = cafe_unique['noun'] + cafe_unique['adj'] + cafe_unique['verb']\n",
    "    \n",
    "    # (9) 카페별 리뷰 dataframe 저장\n",
    "    cafe_unique.to_csv('../data/word2vec_final.csv', index=False)\n",
    "    \n",
    "    return cafe_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806c582a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 벡터 생성 (문장을 구성하는 토큰 벡터의 평균)\n",
    "\n",
    "def get_sentence_mean_vector(token):\n",
    "    vector = []\n",
    "    for i in token:\n",
    "        try:\n",
    "            vector.append(w2v_total.wv[i])\n",
    "        except KeyError as e:\n",
    "            pass\n",
    "    try:\n",
    "        return np.mean(vector, axis=0)\n",
    "    except IndexError as e:\n",
    "        pass\n",
    "\n",
    "cafe_unique['wv_total'] = cafe_unique['token'].map(get_sentence_mean_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04716b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 카페별 벡터 생성 (앞서 정의된 get_sentence_mean_vector 사용)\n",
    "\n",
    "def cafe_to_vector(cafe_unique):\n",
    "    \n",
    "    # (1) Word2Vec을 통해 추출된 토큰의 벡터(150차원) 생성\n",
    "    from gensim.models import Word2Vec\n",
    "    tokens = list(cafe_unique['token'].values)\n",
    "    w2v_total = Word2Vec(sentences=tokens, vector_size=150, window=3, min_count=1, workers=4, sg=1)\n",
    "    \n",
    "    # (2) 카페별 벡터 생성\n",
    "    cafe_unique['wv_total'] = cafe_unique['token'].map(get_sentence_mean_vector)\n",
    "    \n",
    "    return cafe_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e7486d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 카페 유형 분류\n",
    "\n",
    "def cafe_clustering(cafe_unique):\n",
    "    \n",
    "    from sklearn.cluster import KMeans\n",
    "    \n",
    "    word_vectors = cafe_unique.wv_total_noun.to_list() \n",
    "    num_clusters = 4\n",
    "\n",
    "    kmeans_clustering = KMeans(n_clusters = num_clusters, random_state=np.random.RandomState(seed=1))\n",
    "    kmeans_clustering.fit(word_vectors)\n",
    "    idx = kmeans_clustering.fit_predict( word_vectors )\n",
    "    cafe_unique['k=4'] = idx\n",
    "    \n",
    "    return cafe_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae09aaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 카페별 카테고리 점수 산정\n",
    "\n",
    "def cafe_score(cafe_unique, num_clusters):\n",
    "    \n",
    "    # (1) 카페 클러스터별 중심점 \n",
    "\n",
    "    # 감성카페(cluster 0) center point\n",
    "    cluster_0 = kmeans_clustering.cluster_centers_[0]\n",
    "\n",
    "    # 디저트카페 (cluster 3) center point\n",
    "    cluster_1 = kmeans_clustering.cluster_centers_[1]\n",
    "\n",
    "    # 프랜차이즈 카페 center point\n",
    "    cluster_2 = kmeans_clustering.cluster_centers_[2]\n",
    "\n",
    "    # 카공 카페 center point\n",
    "    cluster_3 = kmeans_clustering.cluster_centers_[3]\n",
    "    \n",
    "    # (2) 중심과의 거리 계산\n",
    "    \n",
    "    from scipy.spatial import distance\n",
    "    cafe_unique['감성_dist'] = cafe_unique['wv_total_noun'].apply(lambda x: distance.euclidean(x,cluster_0))\n",
    "    cafe_unique['디저트_dist'] = cafe_unique['wv_total_noun'].apply(lambda x: distance.euclidean(x,cluster_1))\n",
    "    cafe_unique['프랜차이즈_dist'] = cafe_unique['wv_total_noun'].apply(lambda x: distance.euclidean(x,cluster_2))\n",
    "    cafe_unique['공부_dist'] = cafe_unique['wv_total_noun'].apply(lambda x: distance.euclidean(x,cluster_3))\n",
    "    \n",
    "    # (3) 점수화를 위한 거리 scaling\n",
    "    \n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "    # 감성카페\n",
    "    gamsung_scaler = MinMaxScaler()\n",
    "    cafe_unique['감성_scaled'] = gamsung_scaler.fit_transform(cafe_unique[['감성_dist']])\n",
    "    \n",
    "    # 디저트카페\n",
    "    dessert_scaler = MinMaxScaler()\n",
    "    cafe_unique['디저트_scaled'] = dessert_scaler.fit_transform(cafe_unique[['디저트_dist']])\n",
    "    \n",
    "    # 프랜차이즈카페\n",
    "    franchise_scaler = MinMaxScaler()\n",
    "    cafe_unique['프랜차이즈_scaled'] = franchise_scaler.fit_transform(cafe_unique[['프랜차이즈_dist']])\n",
    "    \n",
    "    # 카공 카페\n",
    "    study_scaler = MinMaxScaler()\n",
    "    cafe_unique['공부_scaled'] = study_scaler.fit_transform(cafe_unique[['study_dist']])\n",
    "    \n",
    "    # (4) 최종 점수 계산 : (1-거리)의 제곱\n",
    "    cafe_unique['감성_score'] = cafe_unique['감성_scaled'].apply(lambda x: (1-x)**2)\n",
    "    cafe_unique['디저트_score'] = cafe_unique['dessert_scaled'].apply(lambda x: (1-x)**2)\n",
    "    cafe_unique['프랜차이즈_score'] = cafe_unique['franchise_scaled'].apply(lambda x: (1-x)**2)\n",
    "    cafe_unique['공부_score'] = cafe_unique['study_scaled'].apply(lambda x: (1-x)**2)\n",
    "    \n",
    "    cafe_unique.to_csv('../data/cluster_result.csv')\n",
    "    \n",
    "    return cafe_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2d4733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 지역별 카페 유형 점수 산정\n",
    "\n",
    "def location_cafe_type_score(cafe_unique):\n",
    "    \n",
    "    # (1) 카페별 지역 정보 추출\n",
    "    cafe_unique['si'] = cafe_unique['location'].apply(lambda x: x.split(' ')[0])\n",
    "    cafe_unique['gu'] = cafe_unique['location'].apply(lambda x: x.split(' ')[1])\n",
    "    \n",
    "    # (2) 지역별 카페 유형 점수 산정\n",
    "    location_score = cafe_unique.groupby('gu').agg({'gamsung_score':'mean','gamsung_score':'mean','dessert_score':'mean','franchise_score':'mean', 'study_score':'mean'}).reset_index()\n",
    "    location_score.to_csv('../data/지역별 크롤링 비율')\n",
    "    \n",
    "    return location_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c9dc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "cafe_unique = review_sum()\n",
    "cafe_unique = cafe_to_vector(cafe_unique)\n",
    "cafe_clustering = cafe_clustering(cafe_unique)\n",
    "cafe_unique = cafe_score(cafe_unique)\n",
    "location_score = location_cafe_type_score(cafe_unique)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
